# AI Math & ML Library in C

This repository contains a C-based library built from scratch to handle essential mathematical functions and operations for AI and machine learning projects. It includes implementations of matrix operations, activation functions, loss functions, neural network components, and optimizers. The goal of this project is to provide an in-depth understanding of the underlying mechanics of popular machine learning libraries like TensorFlow and PyTorch.

## Project Overview

This library is structured into different stages of development, starting from basic mathematical functions and gradually advancing to machine learning and deep learning algorithms. It serves both as a learning tool and a foundational library that can be extended and optimized further.

## Stages of Development

### Stage 1: Basic Matrix and Vector Operations
The foundation of machine learning operations begins with matrix and vector computations.
- **Matrix creation, addition, multiplication, and transposition**
- **Dot product for vectors**
- **Scalar multiplication for both matrices and vectors**

### Stage 2: Activation Functions & Loss Functions
These functions are the core of neural networks.
- **Activation Functions**: Sigmoid, ReLU, Tanh, and Softmax
- **Loss Functions**: Mean Squared Error (MSE), Binary Cross-Entropy, Categorical Cross-Entropy

### Stage 3: Neural Network Components
The building blocks of neural networks, including layers and backpropagation.
- **Dense Layer (Fully Connected Layer) forward and backward pass**
- **Basic Optimizers like Gradient Descent and Stochastic Gradient Descent (SGD)**

### Stage 4: Convolutional and Recurrent Neural Networks
Expanding into deep learning concepts such as CNNs and RNNs.
- **2D Convolution Layers**
- **Pooling Layers (Max and Average)**
- **Recurrent Neural Network (RNN) and Long Short-Term Memory (LSTM) cells**

### Stage 5: Advanced Optimizers and Regularization Techniques
Optimization techniques beyond basic gradient descent.
- **Adam and RMSProp optimizers**
- **L2 Regularization and Dropout for preventing overfitting**

### Stage 6: Batch Normalization and Custom Loss Functions
Includes more advanced regularization and customizability for networks.
- **Batch normalization to accelerate training**
- **Custom loss functions for unique use cases**

### Stage 7: Multithreading and GPU Acceleration (Optional)
Enhancing performance by parallel processing and hardware acceleration.
- **Multithreaded matrix multiplication**
- **GPU-accelerated convolutional operations**


